## Raft

### 1、基础知识点

- 状态或角色
  - follower：follwer是集群的初始状态,所有的节点在刚开始加入到集群中,默认是follower的角色,也就是从节点。
  - candidate：候选人。
  - leader：主节点。
- term：任期
  - 每个节点都维护着当前的任期值。
  - 每个节点在检测到自己的任期值低于其他节点会更新自己的任期值，设置为检测到的较高值。
  - 当leader和candidate发现自己的任期低于别的节点，则会立即把自己转换为follower。

### 2、leader选举

- 集群中的节点所有的初始状态都是 `Follower,`然后设置任期(term为0),并启动计时器发起选举(Election)。
- 开始选举之后每个参与方都会有一个随机的超时时间（`Election Timeout`）,这里的关键就是随机 `Timeout`（150ms 到 300ms之间）。
- 最先走完`timeout时间的一个节点开始`进入候选人状态发起投票，向还在 `timeout` 中的另外节点请求投票(Reuest Vote)并等待回复。
- 然后raft会统计得票数,在计数器时间内,得票最多的会成为leader。
- `Leader会定期向所有 `Follower` 发送心跳信息。



**两个节点同时发起选举**

- 它们会同时发起投票，如果最终他们得到的votes是相同的，这个时候raft会等待下一轮的重试，下一轮两个节点的time out可能会不同，重试直到选举出leader。

**leader宕机**

- leader在对follower发送`append entries`的时候，follower会自身维护一个随机时间。
- 如果在超时时间内收到leader的请求就会重置超时时间,如果没有收到超过超时时间,follower没有收到 `Leader`的心跳，follower会认为 `Leader` 可能已经挂了。
- 此时第一个超时的follower会发起投票重新选leader(如果原leader收到请求并不会回复)，当获得集群中超过一半的票数后，follower会成为新leader。
- leader恢复正常后，重新加入集群，身份是follower。

**follower宕机**

-  follower宕机对整个集群影响不大，最多的影响是leader发出的Append Entries无法被收到，但是leader还会继续一直发送,直到follower恢复正常。
- raft会保证发送AppendEntries request的rpc消息是幂等的，如果follower已经接受到了消息，但是leader又让它再次接受，follower会直接忽略。



### 3、Raft如何保证集群的一致性

- Raft协议由leader节点负责接收客户端的请求，leader会将请求再发送给从节点。所以集群强依赖leader节点的可用性，以确保集群数据的一致性。
- 客户端向集群leader提交数据后，leader节点接收到的数据处于未提交状态(Uncommitted)。
- 接着leader节点会并发地向所有follower节点复制数据并等待接收响应ACK。
- leader会等待集群中至少超过一半的节点接收到数据后，再向客户端确认数据已接收。
- 一旦向客户端发出数据接收ACK响应后，表明此时数据状态进入已提交(Committed)，Leader节点再向Follower节点通知该数据状态已提交。
-  follower开始commit自己的数据,此时raft集群达到主节点和从节点的一致。



### 4、集群脑裂问题

- 原因：因为网络分区，集群内部就会被分隔开，在不同的网络分区里因为无法接收到原来leader发出的心跳而超时选主，这样就会造成多leader情况。

- 较小分区会在数据更新的时候因为无法获取集群中的大多数节点的ACK而复制失败。
- 当网络恢复的时候，较小分区leader会因为Term小而成为follower。
- 分区中的所有节点都会回滚自己的数据日志。并匹配新leader的日志。最终集群达到整体一致。



**非对称网络分区**

> 假设有A,B,C,D,E五台机，A是leader，某个时刻A,B出现了分区，但是A,C,D,E以及B,C,D,E都可以互相通信。B在超时没有收到心跳后，把term+1，发起leader选举，如果这段时间C,D,E没有写入更新的日志，由于B的term更大，就会被选为leader，A在后面的RPC中因为自己的term较小也会被降为follower。问题是A成为follower之后又会按照上面B的方式发起选举成为leader，同理B也会再次发起选举，这样周而复始会造成很大的网络开销。

- 这种情况会反复出现新的选举，打断正常的 IO，造成，可用性降低的问题，一般通过`pre-vote`解决。
- 例如，每次发起选举之前，先发起 pre-vote 如果获得大多数 pre-vote 选票，再增大 term 发起选举 vote 投票。为了避免问题描述的情况，其他节点只需要在收到 pre-vote 请求时，判断一下 leader 是否还在，一般实现上，判断最近和 leader 是否正常通信，如果有，那么说明 leader 正常在线，直接拒绝 pre-vote 即可。



[分布式一致性算法raft](https://app.yinxiang.com/shard/s43/nl/13675070/5bbbbed5-e33e-4197-845f-44ff38dfc5ff)





## Paxos

> 在常见的分布式系统中，总会发生诸如**机器宕机**或**网络异常**（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对**某个数据的值**达成**一致**，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。

### 1、基础知识点

- 三种角色：
  - 提议者(Proposer)
  - 接收者(Acceptor)
  - 学习者(Learners)

- Paxos运行过程分为两个阶段，分别是准备阶段(Prepare)和接收阶段(Accept)
- 当某一个提案被过半数的 Acceptor 接受之后，我们就认为当前提案被整个集群接受了。



## POW

- 工作量证明（Proof-of-Work）:是一种对应用服务与资源滥用、或是阻断服务攻击的经济对策。要求用户做耗时适当的复杂运算，并且答案能被服务方快速验算。以确保服务与资源是被真正的需求所使用。



## 分布式一致性问题分类

- **强一致性：** 系统中的某个数据被成功更新后，后续任何对该数据的读取操作都将得到更新后的值。
  - 例如，对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。
  - 保证了强一致性就会损失可用性。
- **弱一致性：** 系统中的某个数据被更新后，后续对该数据的读取操作可能得到更新后的值，也可能是更改前的值。而且后续对该数据的读取也不一定是最新值。
- **最终一致性：** 在一段时间后，节点间的数据会最终达到一致状态。
  - 与弱一致性的区别在于：过了不一致窗口后，后续的读取一定一致。

